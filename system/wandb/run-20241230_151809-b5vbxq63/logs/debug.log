2024-12-30 15:18:09,936 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Current SDK version is 0.19.1
2024-12-30 15:18:09,937 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Configure stats pid to 878631
2024-12-30 15:18:09,937 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from /home/datpd1/.config/wandb/settings
2024-12-30 15:18:09,937 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from /home/datpd1/datpd/capstone/Fed_2024/system/wandb/settings
2024-12-30 15:18:09,937 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from environment variables
2024-12-30 15:18:09,937 INFO    MainThread:878631 [wandb_init.py:_log_setup():528] Logging user logs to /home/datpd1/datpd/capstone/Fed_2024/system/wandb/run-20241230_151809-b5vbxq63/logs/debug.log
2024-12-30 15:18:09,937 INFO    MainThread:878631 [wandb_init.py:_log_setup():529] Logging internal logs to /home/datpd1/datpd/capstone/Fed_2024/system/wandb/run-20241230_151809-b5vbxq63/logs/debug-internal.log
2024-12-30 15:18:09,937 INFO    MainThread:878631 [wandb_init.py:init():644] calling init triggers
2024-12-30 15:18:09,937 INFO    MainThread:878631 [wandb_init.py:init():650] wandb.init called with sweep_config: {}
config: {}
2024-12-30 15:18:09,937 INFO    MainThread:878631 [wandb_init.py:init():675] wandb.init() called when a run is still active
2024-12-30 15:18:09,938 INFO    MainThread:878631 [wandb_run.py:_config_callback():1279] config_cb None None {'goal': 'test', 'device': 'cuda', 'device_id': '0', 'dataset': 'SLEEP', 'num_classes': 12, 'model': 'BaseHeadSplit(\n  (base): HybridBN(\n    (conv1): Sequential(\n      (0): Conv2d(3, 32, kernel_size=(1, 3), stride=(1, 1))\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): MaxPool2d(kernel_size=(1, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (lstm): LSTM(32, 16, batch_first=True)\n    (fc): Identity()\n  )\n  (head): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=64, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=64, out_features=12, bias=True)\n  )\n)', 'batch_size': 64, 'local_learning_rate': 0.01, 'learning_rate_decay': True, 'learning_rate_decay_gamma': 0.99, 'global_rounds': 30, 'local_epochs': 4, 'algorithm': 'FedKDX', 'join_ratio': 0.4, 'random_join_ratio': False, 'num_clients': 15, 'prev': 0, 'times': 1, 'eval_gap': 1, 'save_folder_name': 'items', 'auto_break': False, 'dlg_eval': False, 'dlg_gap': 100, 'batch_num_per_client': 2, 'num_new_clients': 0, 'fine_tuning_epoch_new': 0, 'feature_dim': 512, 'vocab_size': 98635, 'max_len': 200, 'client_drop_rate': 0.0, 'train_slow_rate': 0.0, 'send_slow_rate': 0.0, 'time_select': False, 'time_threthold': 10000, 'beta': 0.0, 'lamda': 1.0, 'mu': 0.0, 'K': 5, 'p_learning_rate': 0.01, 'M': 5, 'itk': 4000, 'alphaK': 1.0, 'sigma': 1.0, 'alpha': 1.0, 'plocal_epochs': 1, 'tau': 1.0, 'fine_tuning_epochs': 10, 'dr_learning_rate': 0.0, 'L': 1.0, 'noise_dim': 512, 'generator_learning_rate': 0.005, 'hidden_dim': 512, 'server_epochs': 1000, 'localize_feature_extractor': False, 'server_learning_rate': 1.0, 'eta': 1.0, 'rand_percent': 80, 'layer_idx': 2, 'mentee_learning_rate': 0.01, 'T_start': 0.95, 'T_end': 0.98, 'momentum': 0.1, 'kl_weight': 0.0, 'gamma': 0.1, 'optimizer': None, 'loss_fn': 'ce', 'wandb': True, 'head': 'Sequential(\n  (0): Linear(in_features=784, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=64, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=64, out_features=12, bias=True)\n)'}
2024-12-30 15:18:09,951 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Current SDK version is 0.19.1
2024-12-30 15:18:09,951 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Configure stats pid to 878631
2024-12-30 15:18:09,952 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from /home/datpd1/.config/wandb/settings
2024-12-30 15:18:09,952 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from /home/datpd1/datpd/capstone/Fed_2024/system/wandb/settings
2024-12-30 15:18:09,952 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from environment variables
2024-12-30 15:18:09,952 INFO    MainThread:878631 [wandb_init.py:_log_setup():528] Logging user logs to /home/datpd1/datpd/capstone/Fed_2024/system/wandb/run-20241230_151809-gj89y637/logs/debug.log
2024-12-30 15:18:09,952 INFO    MainThread:878631 [wandb_init.py:_log_setup():529] Logging internal logs to /home/datpd1/datpd/capstone/Fed_2024/system/wandb/run-20241230_151809-gj89y637/logs/debug-internal.log
2024-12-30 15:18:09,952 INFO    MainThread:878631 [wandb_init.py:init():644] calling init triggers
2024-12-30 15:18:09,952 INFO    MainThread:878631 [wandb_init.py:init():650] wandb.init called with sweep_config: {}
config: {}
2024-12-30 15:18:09,952 INFO    MainThread:878631 [wandb_init.py:init():675] wandb.init() called when a run is still active
2024-12-30 15:18:09,953 INFO    MainThread:878631 [wandb_run.py:_config_callback():1279] config_cb None None {'goal': 'test', 'device': 'cuda', 'device_id': '0', 'dataset': 'SLEEP', 'num_classes': 12, 'model': 'BaseHeadSplit(\n  (base): HybridBN(\n    (conv1): Sequential(\n      (0): Conv2d(3, 32, kernel_size=(1, 3), stride=(1, 1))\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): MaxPool2d(kernel_size=(1, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (lstm): LSTM(32, 16, batch_first=True)\n    (fc): Identity()\n  )\n  (head): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=64, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=64, out_features=12, bias=True)\n  )\n)', 'batch_size': 64, 'local_learning_rate': 0.01, 'learning_rate_decay': True, 'learning_rate_decay_gamma': 0.99, 'global_rounds': 30, 'local_epochs': 4, 'algorithm': 'FedKDX', 'join_ratio': 0.4, 'random_join_ratio': False, 'num_clients': 15, 'prev': 0, 'times': 1, 'eval_gap': 1, 'save_folder_name': 'items', 'auto_break': False, 'dlg_eval': False, 'dlg_gap': 100, 'batch_num_per_client': 2, 'num_new_clients': 0, 'fine_tuning_epoch_new': 0, 'feature_dim': 512, 'vocab_size': 98635, 'max_len': 200, 'client_drop_rate': 0.0, 'train_slow_rate': 0.0, 'send_slow_rate': 0.0, 'time_select': False, 'time_threthold': 10000, 'beta': 0.0, 'lamda': 1.0, 'mu': 0.0, 'K': 5, 'p_learning_rate': 0.01, 'M': 5, 'itk': 4000, 'alphaK': 1.0, 'sigma': 1.0, 'alpha': 1.0, 'plocal_epochs': 1, 'tau': 1.0, 'fine_tuning_epochs': 10, 'dr_learning_rate': 0.0, 'L': 1.0, 'noise_dim': 512, 'generator_learning_rate': 0.005, 'hidden_dim': 512, 'server_epochs': 1000, 'localize_feature_extractor': False, 'server_learning_rate': 1.0, 'eta': 1.0, 'rand_percent': 80, 'layer_idx': 2, 'mentee_learning_rate': 0.01, 'T_start': 0.95, 'T_end': 0.98, 'momentum': 0.1, 'kl_weight': 0.0, 'gamma': 0.1, 'optimizer': None, 'loss_fn': 'ce', 'wandb': True, 'head': 'Sequential(\n  (0): Linear(in_features=784, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=64, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=64, out_features=12, bias=True)\n)'}
2024-12-30 15:18:09,966 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Current SDK version is 0.19.1
2024-12-30 15:18:09,966 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Configure stats pid to 878631
2024-12-30 15:18:09,967 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from /home/datpd1/.config/wandb/settings
2024-12-30 15:18:09,967 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from /home/datpd1/datpd/capstone/Fed_2024/system/wandb/settings
2024-12-30 15:18:09,967 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from environment variables
2024-12-30 15:18:09,967 INFO    MainThread:878631 [wandb_init.py:_log_setup():528] Logging user logs to /home/datpd1/datpd/capstone/Fed_2024/system/wandb/run-20241230_151809-e4rt66c1/logs/debug.log
2024-12-30 15:18:09,967 INFO    MainThread:878631 [wandb_init.py:_log_setup():529] Logging internal logs to /home/datpd1/datpd/capstone/Fed_2024/system/wandb/run-20241230_151809-e4rt66c1/logs/debug-internal.log
2024-12-30 15:18:09,967 INFO    MainThread:878631 [wandb_init.py:init():644] calling init triggers
2024-12-30 15:18:09,967 INFO    MainThread:878631 [wandb_init.py:init():650] wandb.init called with sweep_config: {}
config: {}
2024-12-30 15:18:09,967 INFO    MainThread:878631 [wandb_init.py:init():675] wandb.init() called when a run is still active
2024-12-30 15:18:09,968 INFO    MainThread:878631 [wandb_run.py:_config_callback():1279] config_cb None None {'goal': 'test', 'device': 'cuda', 'device_id': '0', 'dataset': 'SLEEP', 'num_classes': 12, 'model': 'BaseHeadSplit(\n  (base): HybridBN(\n    (conv1): Sequential(\n      (0): Conv2d(3, 32, kernel_size=(1, 3), stride=(1, 1))\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): MaxPool2d(kernel_size=(1, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (lstm): LSTM(32, 16, batch_first=True)\n    (fc): Identity()\n  )\n  (head): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=64, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=64, out_features=12, bias=True)\n  )\n)', 'batch_size': 64, 'local_learning_rate': 0.01, 'learning_rate_decay': True, 'learning_rate_decay_gamma': 0.99, 'global_rounds': 30, 'local_epochs': 4, 'algorithm': 'FedKDX', 'join_ratio': 0.4, 'random_join_ratio': False, 'num_clients': 15, 'prev': 0, 'times': 1, 'eval_gap': 1, 'save_folder_name': 'items', 'auto_break': False, 'dlg_eval': False, 'dlg_gap': 100, 'batch_num_per_client': 2, 'num_new_clients': 0, 'fine_tuning_epoch_new': 0, 'feature_dim': 512, 'vocab_size': 98635, 'max_len': 200, 'client_drop_rate': 0.0, 'train_slow_rate': 0.0, 'send_slow_rate': 0.0, 'time_select': False, 'time_threthold': 10000, 'beta': 0.0, 'lamda': 1.0, 'mu': 0.0, 'K': 5, 'p_learning_rate': 0.01, 'M': 5, 'itk': 4000, 'alphaK': 1.0, 'sigma': 1.0, 'alpha': 1.0, 'plocal_epochs': 1, 'tau': 1.0, 'fine_tuning_epochs': 10, 'dr_learning_rate': 0.0, 'L': 1.0, 'noise_dim': 512, 'generator_learning_rate': 0.005, 'hidden_dim': 512, 'server_epochs': 1000, 'localize_feature_extractor': False, 'server_learning_rate': 1.0, 'eta': 1.0, 'rand_percent': 80, 'layer_idx': 2, 'mentee_learning_rate': 0.01, 'T_start': 0.95, 'T_end': 0.98, 'momentum': 0.1, 'kl_weight': 0.0, 'gamma': 0.1, 'optimizer': None, 'loss_fn': 'ce', 'wandb': True, 'head': 'Sequential(\n  (0): Linear(in_features=784, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=64, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=64, out_features=12, bias=True)\n)'}
2024-12-30 15:18:09,986 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Current SDK version is 0.19.1
2024-12-30 15:18:09,986 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Configure stats pid to 878631
2024-12-30 15:18:09,986 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from /home/datpd1/.config/wandb/settings
2024-12-30 15:18:09,986 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from /home/datpd1/datpd/capstone/Fed_2024/system/wandb/settings
2024-12-30 15:18:09,986 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from environment variables
2024-12-30 15:18:09,987 INFO    MainThread:878631 [wandb_init.py:_log_setup():528] Logging user logs to /home/datpd1/datpd/capstone/Fed_2024/system/wandb/run-20241230_151809-zjkb7s87/logs/debug.log
2024-12-30 15:18:09,987 INFO    MainThread:878631 [wandb_init.py:_log_setup():529] Logging internal logs to /home/datpd1/datpd/capstone/Fed_2024/system/wandb/run-20241230_151809-zjkb7s87/logs/debug-internal.log
2024-12-30 15:18:09,987 INFO    MainThread:878631 [wandb_init.py:init():644] calling init triggers
2024-12-30 15:18:09,987 INFO    MainThread:878631 [wandb_init.py:init():650] wandb.init called with sweep_config: {}
config: {}
2024-12-30 15:18:09,987 INFO    MainThread:878631 [wandb_init.py:init():675] wandb.init() called when a run is still active
2024-12-30 15:18:09,988 INFO    MainThread:878631 [wandb_run.py:_config_callback():1279] config_cb None None {'goal': 'test', 'device': 'cuda', 'device_id': '0', 'dataset': 'SLEEP', 'num_classes': 12, 'model': 'BaseHeadSplit(\n  (base): HybridBN(\n    (conv1): Sequential(\n      (0): Conv2d(3, 32, kernel_size=(1, 3), stride=(1, 1))\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): MaxPool2d(kernel_size=(1, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (lstm): LSTM(32, 16, batch_first=True)\n    (fc): Identity()\n  )\n  (head): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=64, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=64, out_features=12, bias=True)\n  )\n)', 'batch_size': 64, 'local_learning_rate': 0.01, 'learning_rate_decay': True, 'learning_rate_decay_gamma': 0.99, 'global_rounds': 30, 'local_epochs': 4, 'algorithm': 'FedKDX', 'join_ratio': 0.4, 'random_join_ratio': False, 'num_clients': 15, 'prev': 0, 'times': 1, 'eval_gap': 1, 'save_folder_name': 'items', 'auto_break': False, 'dlg_eval': False, 'dlg_gap': 100, 'batch_num_per_client': 2, 'num_new_clients': 0, 'fine_tuning_epoch_new': 0, 'feature_dim': 512, 'vocab_size': 98635, 'max_len': 200, 'client_drop_rate': 0.0, 'train_slow_rate': 0.0, 'send_slow_rate': 0.0, 'time_select': False, 'time_threthold': 10000, 'beta': 0.0, 'lamda': 1.0, 'mu': 0.0, 'K': 5, 'p_learning_rate': 0.01, 'M': 5, 'itk': 4000, 'alphaK': 1.0, 'sigma': 1.0, 'alpha': 1.0, 'plocal_epochs': 1, 'tau': 1.0, 'fine_tuning_epochs': 10, 'dr_learning_rate': 0.0, 'L': 1.0, 'noise_dim': 512, 'generator_learning_rate': 0.005, 'hidden_dim': 512, 'server_epochs': 1000, 'localize_feature_extractor': False, 'server_learning_rate': 1.0, 'eta': 1.0, 'rand_percent': 80, 'layer_idx': 2, 'mentee_learning_rate': 0.01, 'T_start': 0.95, 'T_end': 0.98, 'momentum': 0.1, 'kl_weight': 0.0, 'gamma': 0.1, 'optimizer': None, 'loss_fn': 'ce', 'wandb': True, 'head': 'Sequential(\n  (0): Linear(in_features=784, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=64, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=64, out_features=12, bias=True)\n)'}
2024-12-30 15:18:10,005 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Current SDK version is 0.19.1
2024-12-30 15:18:10,005 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Configure stats pid to 878631
2024-12-30 15:18:10,006 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from /home/datpd1/.config/wandb/settings
2024-12-30 15:18:10,006 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from /home/datpd1/datpd/capstone/Fed_2024/system/wandb/settings
2024-12-30 15:18:10,006 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from environment variables
2024-12-30 15:18:10,006 INFO    MainThread:878631 [wandb_init.py:_log_setup():528] Logging user logs to /home/datpd1/datpd/capstone/Fed_2024/system/wandb/run-20241230_151810-dwdoc5cg/logs/debug.log
2024-12-30 15:18:10,006 INFO    MainThread:878631 [wandb_init.py:_log_setup():529] Logging internal logs to /home/datpd1/datpd/capstone/Fed_2024/system/wandb/run-20241230_151810-dwdoc5cg/logs/debug-internal.log
2024-12-30 15:18:10,006 INFO    MainThread:878631 [wandb_init.py:init():644] calling init triggers
2024-12-30 15:18:10,006 INFO    MainThread:878631 [wandb_init.py:init():650] wandb.init called with sweep_config: {}
config: {}
2024-12-30 15:18:10,007 INFO    MainThread:878631 [wandb_init.py:init():675] wandb.init() called when a run is still active
2024-12-30 15:18:10,007 INFO    MainThread:878631 [wandb_run.py:_config_callback():1279] config_cb None None {'goal': 'test', 'device': 'cuda', 'device_id': '0', 'dataset': 'SLEEP', 'num_classes': 12, 'model': 'BaseHeadSplit(\n  (base): HybridBN(\n    (conv1): Sequential(\n      (0): Conv2d(3, 32, kernel_size=(1, 3), stride=(1, 1))\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): MaxPool2d(kernel_size=(1, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (lstm): LSTM(32, 16, batch_first=True)\n    (fc): Identity()\n  )\n  (head): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=64, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=64, out_features=12, bias=True)\n  )\n)', 'batch_size': 64, 'local_learning_rate': 0.01, 'learning_rate_decay': True, 'learning_rate_decay_gamma': 0.99, 'global_rounds': 30, 'local_epochs': 4, 'algorithm': 'FedKDX', 'join_ratio': 0.4, 'random_join_ratio': False, 'num_clients': 15, 'prev': 0, 'times': 1, 'eval_gap': 1, 'save_folder_name': 'items', 'auto_break': False, 'dlg_eval': False, 'dlg_gap': 100, 'batch_num_per_client': 2, 'num_new_clients': 0, 'fine_tuning_epoch_new': 0, 'feature_dim': 512, 'vocab_size': 98635, 'max_len': 200, 'client_drop_rate': 0.0, 'train_slow_rate': 0.0, 'send_slow_rate': 0.0, 'time_select': False, 'time_threthold': 10000, 'beta': 0.0, 'lamda': 1.0, 'mu': 0.0, 'K': 5, 'p_learning_rate': 0.01, 'M': 5, 'itk': 4000, 'alphaK': 1.0, 'sigma': 1.0, 'alpha': 1.0, 'plocal_epochs': 1, 'tau': 1.0, 'fine_tuning_epochs': 10, 'dr_learning_rate': 0.0, 'L': 1.0, 'noise_dim': 512, 'generator_learning_rate': 0.005, 'hidden_dim': 512, 'server_epochs': 1000, 'localize_feature_extractor': False, 'server_learning_rate': 1.0, 'eta': 1.0, 'rand_percent': 80, 'layer_idx': 2, 'mentee_learning_rate': 0.01, 'T_start': 0.95, 'T_end': 0.98, 'momentum': 0.1, 'kl_weight': 0.0, 'gamma': 0.1, 'optimizer': None, 'loss_fn': 'ce', 'wandb': True, 'head': 'Sequential(\n  (0): Linear(in_features=784, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=64, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=64, out_features=12, bias=True)\n)'}
2024-12-30 15:18:10,024 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Current SDK version is 0.19.1
2024-12-30 15:18:10,024 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Configure stats pid to 878631
2024-12-30 15:18:10,024 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from /home/datpd1/.config/wandb/settings
2024-12-30 15:18:10,024 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from /home/datpd1/datpd/capstone/Fed_2024/system/wandb/settings
2024-12-30 15:18:10,025 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from environment variables
2024-12-30 15:18:10,025 INFO    MainThread:878631 [wandb_init.py:_log_setup():528] Logging user logs to /home/datpd1/datpd/capstone/Fed_2024/system/wandb/run-20241230_151810-30ong5pg/logs/debug.log
2024-12-30 15:18:10,025 INFO    MainThread:878631 [wandb_init.py:_log_setup():529] Logging internal logs to /home/datpd1/datpd/capstone/Fed_2024/system/wandb/run-20241230_151810-30ong5pg/logs/debug-internal.log
2024-12-30 15:18:10,025 INFO    MainThread:878631 [wandb_init.py:init():644] calling init triggers
2024-12-30 15:18:10,025 INFO    MainThread:878631 [wandb_init.py:init():650] wandb.init called with sweep_config: {}
config: {}
2024-12-30 15:18:10,025 INFO    MainThread:878631 [wandb_init.py:init():675] wandb.init() called when a run is still active
2024-12-30 15:18:10,026 INFO    MainThread:878631 [wandb_run.py:_config_callback():1279] config_cb None None {'goal': 'test', 'device': 'cuda', 'device_id': '0', 'dataset': 'SLEEP', 'num_classes': 12, 'model': 'BaseHeadSplit(\n  (base): HybridBN(\n    (conv1): Sequential(\n      (0): Conv2d(3, 32, kernel_size=(1, 3), stride=(1, 1))\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): MaxPool2d(kernel_size=(1, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (lstm): LSTM(32, 16, batch_first=True)\n    (fc): Identity()\n  )\n  (head): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=64, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=64, out_features=12, bias=True)\n  )\n)', 'batch_size': 64, 'local_learning_rate': 0.01, 'learning_rate_decay': True, 'learning_rate_decay_gamma': 0.99, 'global_rounds': 30, 'local_epochs': 4, 'algorithm': 'FedKDX', 'join_ratio': 0.4, 'random_join_ratio': False, 'num_clients': 15, 'prev': 0, 'times': 1, 'eval_gap': 1, 'save_folder_name': 'items', 'auto_break': False, 'dlg_eval': False, 'dlg_gap': 100, 'batch_num_per_client': 2, 'num_new_clients': 0, 'fine_tuning_epoch_new': 0, 'feature_dim': 512, 'vocab_size': 98635, 'max_len': 200, 'client_drop_rate': 0.0, 'train_slow_rate': 0.0, 'send_slow_rate': 0.0, 'time_select': False, 'time_threthold': 10000, 'beta': 0.0, 'lamda': 1.0, 'mu': 0.0, 'K': 5, 'p_learning_rate': 0.01, 'M': 5, 'itk': 4000, 'alphaK': 1.0, 'sigma': 1.0, 'alpha': 1.0, 'plocal_epochs': 1, 'tau': 1.0, 'fine_tuning_epochs': 10, 'dr_learning_rate': 0.0, 'L': 1.0, 'noise_dim': 512, 'generator_learning_rate': 0.005, 'hidden_dim': 512, 'server_epochs': 1000, 'localize_feature_extractor': False, 'server_learning_rate': 1.0, 'eta': 1.0, 'rand_percent': 80, 'layer_idx': 2, 'mentee_learning_rate': 0.01, 'T_start': 0.95, 'T_end': 0.98, 'momentum': 0.1, 'kl_weight': 0.0, 'gamma': 0.1, 'optimizer': None, 'loss_fn': 'ce', 'wandb': True, 'head': 'Sequential(\n  (0): Linear(in_features=784, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=64, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=64, out_features=12, bias=True)\n)'}
2024-12-30 15:18:10,042 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Current SDK version is 0.19.1
2024-12-30 15:18:10,043 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Configure stats pid to 878631
2024-12-30 15:18:10,043 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from /home/datpd1/.config/wandb/settings
2024-12-30 15:18:10,043 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from /home/datpd1/datpd/capstone/Fed_2024/system/wandb/settings
2024-12-30 15:18:10,043 INFO    MainThread:878631 [wandb_setup.py:_flush():68] Loading settings from environment variables
2024-12-30 15:18:10,043 INFO    MainThread:878631 [wandb_init.py:_log_setup():528] Logging user logs to /home/datpd1/datpd/capstone/Fed_2024/system/wandb/run-20241230_151810-7gase8k0/logs/debug.log
2024-12-30 15:18:10,043 INFO    MainThread:878631 [wandb_init.py:_log_setup():529] Logging internal logs to /home/datpd1/datpd/capstone/Fed_2024/system/wandb/run-20241230_151810-7gase8k0/logs/debug-internal.log
2024-12-30 15:18:10,044 INFO    MainThread:878631 [wandb_init.py:init():644] calling init triggers
2024-12-30 15:18:10,044 INFO    MainThread:878631 [wandb_init.py:init():650] wandb.init called with sweep_config: {}
config: {}
2024-12-30 15:18:10,044 INFO    MainThread:878631 [wandb_init.py:init():675] wandb.init() called when a run is still active
2024-12-30 15:18:10,045 INFO    MainThread:878631 [wandb_run.py:_config_callback():1279] config_cb None None {'goal': 'test', 'device': 'cuda', 'device_id': '0', 'dataset': 'SLEEP', 'num_classes': 12, 'model': 'BaseHeadSplit(\n  (base): HybridBN(\n    (conv1): Sequential(\n      (0): Conv2d(3, 32, kernel_size=(1, 3), stride=(1, 1))\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU()\n      (3): MaxPool2d(kernel_size=(1, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (lstm): LSTM(32, 16, batch_first=True)\n    (fc): Identity()\n  )\n  (head): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=64, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=64, out_features=12, bias=True)\n  )\n)', 'batch_size': 64, 'local_learning_rate': 0.01, 'learning_rate_decay': True, 'learning_rate_decay_gamma': 0.99, 'global_rounds': 30, 'local_epochs': 4, 'algorithm': 'FedKDX', 'join_ratio': 0.4, 'random_join_ratio': False, 'num_clients': 15, 'prev': 0, 'times': 1, 'eval_gap': 1, 'save_folder_name': 'items', 'auto_break': False, 'dlg_eval': False, 'dlg_gap': 100, 'batch_num_per_client': 2, 'num_new_clients': 0, 'fine_tuning_epoch_new': 0, 'feature_dim': 512, 'vocab_size': 98635, 'max_len': 200, 'client_drop_rate': 0.0, 'train_slow_rate': 0.0, 'send_slow_rate': 0.0, 'time_select': False, 'time_threthold': 10000, 'beta': 0.0, 'lamda': 1.0, 'mu': 0.0, 'K': 5, 'p_learning_rate': 0.01, 'M': 5, 'itk': 4000, 'alphaK': 1.0, 'sigma': 1.0, 'alpha': 1.0, 'plocal_epochs': 1, 'tau': 1.0, 'fine_tuning_epochs': 10, 'dr_learning_rate': 0.0, 'L': 1.0, 'noise_dim': 512, 'generator_learning_rate': 0.005, 'hidden_dim': 512, 'server_epochs': 1000, 'localize_feature_extractor': False, 'server_learning_rate': 1.0, 'eta': 1.0, 'rand_percent': 80, 'layer_idx': 2, 'mentee_learning_rate': 0.01, 'T_start': 0.95, 'T_end': 0.98, 'momentum': 0.1, 'kl_weight': 0.0, 'gamma': 0.1, 'optimizer': None, 'loss_fn': 'ce', 'wandb': True, 'head': 'Sequential(\n  (0): Linear(in_features=784, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=64, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=64, out_features=12, bias=True)\n)'}
2024-12-30 15:19:39,214 WARNING MsgRouterThr:878631 [router.py:message_loop():75] message_loop has been closed
